{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UE6KxmrVhch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as snb\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import fuzz\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import tensorflow_hub as hub\n",
        "import random as rn\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import XLNetConfig, TFXLNetModel, XLNetTokenizer, TFXLNetMainLayer\n",
        "import tensorflow_hub as hub\n",
        "from scipy.stats import spearmanr\n",
        "import time\n",
        "import os\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bboRXiGSVmVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XLNet(TFXLNetModel):\n",
        "    def __init__(self, config, *inputs, **kwrgs):\n",
        "        super(XLNet, self).__init__(config, *inputs, **kwrgs)\n",
        "        self.transformer = TFXLNetMainLayer(config, name = 'transformer')\n",
        "        for i in range(1, 3):\n",
        "            self.transformer.layer[-i].trainable = False\n",
        "    def call(self, inputs, **kwrgs):\n",
        "        outputs = self.transformer(inputs)\n",
        "        hidden_states = outputs[1]\n",
        "        h12 = hidden_states[-1][:, 0, :]\n",
        "        h11 = hidden_states[-2][:, 0, :]\n",
        "        h10 = hidden_states[-3][:, 0, :]\n",
        "        h9 = hidden_states[-4][:, 0, :]\n",
        "        concat = tf.keras.layers.Concatenate(axis = -1)([h9, h10, h11, h12])\n",
        "        return concat\n",
        "def transformer_model(name):\n",
        "    id_1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "    id_2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "\n",
        "    type_id_1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "    type_id_2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "\n",
        "    a1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "    a2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n",
        "\n",
        "    config = XLNetConfig.from_pretrained('xlnet/config-xlnet-base-cased', output_hidden_states = True)\n",
        "    transformer = XLNet.from_pretrained('xlnet/model-xlnet-base-cased', config = config)\n",
        "\n",
        "  \n",
        "    out_1 = transformer({'input_ids':id_1, 'attention_mask':a1, 'token_type_ids':type_id_1})\n",
        "    out_2 = transformer({'input_ids':id_2, 'attention_mask':a2, 'token_type_ids':type_id_2})\n",
        "  \n",
        "    concat = tf.keras.layers.Concatenate(axis = -1)([out_1, out_2])\n",
        "    drop = tf.keras.layers.Dropout(rate = 0.1)(concat)\n",
        "    dense = tf.keras.layers.Dense(30, activation = 'sigmoid')(drop)\n",
        "    return tf.keras.Model(inputs = [id_1, id_2, type_id_1, type_id_2, a1, a2], outputs = [dense])\n",
        "def SpearmanCorrCoeff(A, B):\n",
        "        overall_score = []\n",
        "        for index in range(A.shape[1]):\n",
        "            overall_score += [spearmanr(A[:, index], B[:, index]).correlation]\n",
        "        return overall_score"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul7sVsdfVI1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimize:\n",
        "    def __init__(self):\n",
        "        self.clips = [[0, 1] for i in range(30)]\n",
        "        self.ab_ = [(0, 0.15), (0.85, 1)]\n",
        "        self.new_scores, self.scores = (None, None)\n",
        "    def fit(self, labels, preds):\n",
        "        self.scores = [SpearmanCorrCoeff(labels[:, i:i+1], preds[:, i:i+1]) for i in range(30)]\n",
        "        for i in range(30):\n",
        "            self.golden_section_search(labels[:, i:i+1], preds[:, i:i+1], i, 0)\n",
        "            self.golden_section_search(labels[:, i:i+1], preds[:, i:i+1], i, 1)\n",
        "        self.new_scores = [np.nan_to_num(SpearmanCorrCoeff(labels[:, i:i+1], np.clip(preds[:, i:i+1], self.clips[i][0], self.clips[i][1]))) for i in range(30)]\n",
        "        for i in range(30):\n",
        "            if self.scores[i] >= self.new_scores[i]:\n",
        "                self.clips[i] = [0, 1]\n",
        "    def golden_section_search(self, labels, preds, i, idx):\n",
        "        (a, b) = self.ab_[idx]\n",
        "        c = 0.618\n",
        "        x1 = b - c*(b-a)\n",
        "        x2 = (b-a)*c + a\n",
        "        \n",
        "        for epochs in range(20):\n",
        "            self.clips[i][idx] = x1\n",
        "            score_a = -self.score(labels, preds, i)\n",
        "            self.clips[i][idx] = x2\n",
        "            score_b = -self.score(labels, preds, i)\n",
        "            if np.isnan(score_a):\n",
        "                score_a = 0\n",
        "            elif np.isnan(score_b):\n",
        "                score_b = 0\n",
        "            elif score_a <= score_b:\n",
        "                b = x2\n",
        "                x2 = x1\n",
        "                x1 = b - c*(b-a)\n",
        "            else:\n",
        "                a = x1\n",
        "                x1 = x2\n",
        "                x2 = (b-a)*c + a\n",
        "        \n",
        "        self.clips[i][idx] = x1\n",
        "        score_x1 = self.score(labels, preds, i)\n",
        "        self.clips[i][idx] = x2\n",
        "        score_x2 = self.score(labels, preds, i)\n",
        "        if score_x1 > score_x2:\n",
        "            self.clips[i][idx] = x1\n",
        "        else:\n",
        "            self.clips[i][idx] = x2\n",
        "                    \n",
        "            \n",
        "    def score(self, labels, preds, i):\n",
        "        return SpearmanCorrCoeff(labels, np.clip(preds, self.clips[i][0], self.clips[i][1]))\n",
        "    def transform(self, preds):\n",
        "        temp = preds.copy()\n",
        "        for i in range(30):\n",
        "            clipped = np.clip(preds[:, i], self.clips[i][0], self.clips[i][1])\n",
        "            if np.unique(clipped).shape[0] > 1:\n",
        "                temp[:, i][:] = clipped\n",
        "        return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMyyJfgXNklJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs(df):\n",
        "    '''This function is used to preprocess and model-ready inputs from text data.'''\n",
        "\n",
        "    contractions = {\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\n",
        "    \"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\"I'd\": \"I would\",\"i'd've\": \"I would have\",\"i'll\": \"I will\",\n",
        "    \"i'll've\": \"I will have\",\"i'm\": \"I am\",\"i've\": \"I have\",\"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\"isn't\": \"is not\",\"its\":\"it is\",\"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \n",
        "    \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    rules = {\n",
        "        \"'t\": \" not\",\"'cause\": \" because\",\"'ve\": \" have\",\"'t\": \" not\",\"'s\": \" is\",\"'d\": \" had\"\n",
        "    }\n",
        "\n",
        "    punctuations = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", \n",
        "                    '$', '&', '/', '[', ']','>', '%', '=', '#', '*', '+', '\\\\', '•', '~', \n",
        "                    '@', '£', '·', '_', '{', '}', '©', '^','®', '`', '<', '→', '°', '€', '™', \n",
        "                    '›', '♥', '←', '×', '§', '″', '′', 'Â', '█','½', 'à', '…', '“', '★', '”', \n",
        "                    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶','↑', '±', '¿', '▾', '═', \n",
        "                    '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼','⊕', '▼', '▪', '†',\n",
        "                    '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲','è', '¸', '¾', \n",
        "                    'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪','╩', \n",
        "                    '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', \n",
        "                    'ï', 'Ø', '¹', '≤', '‡', '√'] + list(string.punctuation)\n",
        "    token_replace = {\n",
        "        'usepackage':'latex','orf19':'gene','documentclass':'latex','magento':'open-source e-commerce',\n",
        "        'appium':'web-app','tikz':'programming language',\n",
        "        'tikzpicture':'programming language','openvpn':'vpn','httpclient':'http client',\n",
        "        'arraylist':'array list','jsonobject': 'json',\n",
        "        'artifactid':'xml','hwnd':'os'\n",
        "    }\n",
        "    punctuations_in_embeddings = \\\n",
        "    {',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', \n",
        "                    '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '{', '}', '©', '^', '®', \n",
        "                    '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        "                    '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '═', \n",
        "                    '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', \n",
        "                    '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        "                    '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔',\n",
        "                    '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', \n",
        "                    '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '{', \n",
        "                    '|', '}', '~'}\n",
        "    \n",
        "    '''Pipeline for text preprocessing. Check coverage computes what %age of tokens in the text data \n",
        "        are covered by Embeddings. preprocess_text is used to clean text.\n",
        "    '''\n",
        "    def improve_text(dframes):\n",
        "        def preprocess_text(s):\n",
        "            s = s.lower()\n",
        "            '''Expanding contractions...'''\n",
        "            for key, value in contractions.items():\n",
        "                s = s.replace(key, f' {value} ')\n",
        "            for key, value in rules.items():\n",
        "                s = s.replace(key, f' {value} ')\n",
        "            '''Fixing punctuations...'''\n",
        "            for punct in punctuations:\n",
        "                if punct in punctuations_in_embeddings:\n",
        "                    s = s.replace(punct, f' {punct} ')\n",
        "                else:\n",
        "                    s = s.replace(punct, ' ')\n",
        "            '''Replacing few tokens with its similar word/group of words...'''\n",
        "            for key, value in token_replace.items():\n",
        "                s = s.replace(key, value)\n",
        "            '''Removing HTML tags'''\n",
        "            s = re.sub('<.*?>', ' ', s)\n",
        "            s = re.sub('\\s+', ' ', s)\n",
        "            return s\n",
        "        print(\"Applying Preprocessing.....\")\n",
        "        for each_df in dframes:\n",
        "            each_df['clean_title'] = each_df['question_title'].apply(preprocess_text)\n",
        "            each_df['clean_question_body'] = each_df['question_body'].apply(preprocess_text)\n",
        "            each_df['clean_answer'] = each_df['answer'].apply(preprocess_text)\n",
        "        print(\"Preprocessing Finished\")\n",
        "\n",
        "    def tokenize_input(tokenizer, s1, s2, tags, data_name, max_length):\n",
        "        if s2 is not None:\n",
        "            x = tokenizer.encode_plus(s1, s2, pad_to_max_length=False)\n",
        "            if len(x['input_ids']) > max_length:\n",
        "                segment_1 = int(0.25*max_length)\n",
        "                x['input_ids'] = x['input_ids'][:segment_1] + x['input_ids'][-(max_length-segment_1):]\n",
        "                x['attention_mask'] = x['attention_mask'][:segment_1] + x['attention_mask'][-(max_length-segment_1):]\n",
        "                x['token_type_ids'] = x['token_type_ids'][:segment_1] + x['token_type_ids'][-(max_length-segment_1):]\n",
        "            else:\n",
        "                diff = max_length - len(x['input_ids'])\n",
        "                x['input_ids'] = [tokenizer.pad_token_id]*diff + x['input_ids']\n",
        "                x['attention_mask'] = [0]*diff + x['attention_mask']\n",
        "                x['token_type_ids'] = [tokenizer.pad_token_type_id]*diff + x['token_type_ids']\n",
        "\n",
        "        else:\n",
        "            x = tokenizer.encode_plus(s1)\n",
        "            if len(x['input_ids']) > max_length:\n",
        "                segment_1 = int(0.25*max_length)\n",
        "                x['input_ids'] = x['input_ids'][:segment_1] + x['input_ids'][-(max_length-segment_1):]\n",
        "                x['attention_mask'] = x['attention_mask'][:segment_1] + x['attention_mask'][-(max_length-segment_1):]\n",
        "                x['token_type_ids'] = x['token_type_ids'][:segment_1] + x['token_type_ids'][-(max_length-segment_1):]\n",
        "            else:\n",
        "                diff = max_length - len(x['input_ids'])\n",
        "                x['input_ids'] = [tokenizer.pad_token_id]*diff + x['input_ids']\n",
        "                x['attention_mask'] = [0]*diff + x['attention_mask']\n",
        "                x['token_type_ids'] = [tokenizer.pad_token_type_id]*diff + x['token_type_ids']\n",
        "\n",
        "        data[data_name][tags[0]].append(x['input_ids']) \n",
        "        data[data_name][tags[1]].append(x['token_type_ids'])\n",
        "        data[data_name][tags[2]].append(x['attention_mask']) \n",
        "\n",
        "    def is_eng(x):\n",
        "        if ((x == 'english') | (x == 'eli')):\n",
        "            return 1\n",
        "        return 0\n",
        "    def question_type(row):\n",
        "        if ((row['is_eng'] == 1) & (row['a_count'] > 95)):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    def fetch_words(s, use_stopwords = False, ignore_stopwords = False):\n",
        "        if ignore_stopwords == True:\n",
        "            return {word for word in s.split() if word not in stop_words}\n",
        "        elif use_stopwords:\n",
        "            return {word for word in s.split() if word in stop_words}\n",
        "        else:\n",
        "            return set(s.split())\n",
        "    def text_features(row, use_stopwords, is_token, return_max):\n",
        "        t1, t2 = None, None\n",
        "        if use_stopwords == True:\n",
        "            t1 = fetch_words(row[0], use_stopwords = True)\n",
        "            t2 = fetch_words(row[1], use_stopwords = True)\n",
        "        elif is_token == True:\n",
        "            t1 = fetch_words(row[0], ignore_stopwords = True)\n",
        "            t2 = fetch_words(row[1], ignore_stopwords = True)\n",
        "        else:\n",
        "            t1 = fetch_words(row[0])\n",
        "            t2 = fetch_words(row[1])\n",
        "        if return_max:\n",
        "            try:\n",
        "                ans = len(t1.intersection(t2))/max(len(t1), len(t2))\n",
        "            except:\n",
        "                return 0\n",
        "        else:\n",
        "            try:\n",
        "                ans = len(t1.intersection(t2))/min(len(t1), len(t2))\n",
        "            except:\n",
        "                return 0\n",
        "        return ans\n",
        "\n",
        "    def get_ratio(row, is_token = False):\n",
        "        if is_token == True:\n",
        "            t1 = len(set([word for word in row[0].split() if word not in stop_words]))\n",
        "            t2 = len(set([word for word in row[1].split() if word not in stop_words]))\n",
        "        t1 = len(set([word for word in row[0].split()]))\n",
        "        t2 = len(set([word for word in row[1].split()]))\n",
        "        try:\n",
        "            return t1/t2\n",
        "        except:\n",
        "            return 0\n",
        "    def get_fuzzy_ratio(row, ratio, partial_ratio, token_sort_ratio, token_set_ratio):\n",
        "        if ratio:\n",
        "            return fuzz.ratio(row[0], row[1])/100\n",
        "        elif partial_ratio:\n",
        "            return fuzz.partial_ratio(row[0], row[1])/100\n",
        "        elif token_sort_ratio:\n",
        "            return fuzz.token_sort_ratio(row[0], row[1])/100\n",
        "        else:\n",
        "            return fuzz.token_set_ratio(row[0], row[1])/100\n",
        "        \n",
        "    def get_polarity(s):\n",
        "        return TextBlob(s).sentiment.polarity\n",
        "    def get_token_ids(texts, max_length):\n",
        "        tokens = []\n",
        "        for text in texts:\n",
        "            tmp_tokens = []\n",
        "            if len(text.split()) > max_length:\n",
        "                for each in (text.split()[:(max_length//2)] + text.split()[-(max_length//2):]):\n",
        "                    tmp_tokens.append(vocab['token2id'].get(each, 0))\n",
        "                tokens.append(tmp_tokens)\n",
        "            else:\n",
        "                for each in (text.split()[:max_length]):\n",
        "                    tmp_tokens.append(vocab['token2id'].get(each, 0))\n",
        "                tokens.append(tmp_tokens)\n",
        "        return tf.keras.preprocessing.sequence.pad_sequences(tokens, padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    feature_engg_cols = ['cwc_min_title_question_body',\n",
        "       'cwc_max_title_question_body', 'csc_min_title_question_body',\n",
        "       'csc_max_title_question_body', 'ctc_min_title_question_body',\n",
        "       'ctc_max_title_question_body', 'cwc_min_title_answer',\n",
        "       'cwc_max_title_answer', 'csc_min_title_answer', 'csc_max_title_answer',\n",
        "       'ctc_min_title_answer', 'ctc_max_title_answer',\n",
        "       'cwc_min_question_body_answer', 'cwc_max_question_body_answer',\n",
        "       'csc_min_question_body_answer', 'csc_max_question_body_answer',\n",
        "       'ctc_min_question_body_answer', 'ctc_max_question_body_answer',\n",
        "       'word_title_question_body_ratio', 'token_title_question_body_ratio',\n",
        "       'word_question_body_answer_ratio', 'token_question_body_answer_ratio',\n",
        "       'word_title_answer_ratio', 'token_title_answer_ratio',\n",
        "       'ratio_title_question_body', 'partial_ratio_title_question_body',\n",
        "       'token_sort_ratio_title_question_body',\n",
        "       'token_set_ratio_title_question_body', 'ratio_question_body_answer',\n",
        "       'partial_ratio_question_body_answer',\n",
        "       'token_sort_ratio_question_body_answer',\n",
        "       'token_set_ratio_question_body_answer', 'ratio_title_answer',\n",
        "       'partial_ratio_title_answer', 'token_sort_ratio_title_answer',\n",
        "       'token_set_ratio_title_answer', 'question_title_polarity', 'question_body_polarity', 'answer_polarity']\n",
        "    \n",
        "    improve_text([df])\n",
        "    \n",
        "    df['domain'] = df.host.apply(lambda s:s.split('.')[0])\n",
        "    df['is_eng'] = df.domain.apply(lambda x:is_eng(x))\n",
        "    df['a_count'] = df.answer.apply(lambda x: len(x.split()))\n",
        "    df['question_type_spelling_modified'] = df[['is_eng', 'a_count']].apply(question_type, axis = 1)\n",
        "    \n",
        "    df['question_title_polarity'] = df.clean_title.apply(get_polarity)\n",
        "    df['question_body_polarity'] = df.clean_question_body.apply(get_polarity)\n",
        "    df['answer_polarity'] = df.clean_answer.apply(get_polarity)\n",
        "    df['question_title_count'] = df.clean_title.apply(lambda x: len(x.split()))\n",
        "    df['question_body_count'] = df.clean_question_body.apply(lambda x: len(x.split()))\n",
        "    df['answer_count'] = df.clean_answer.apply(lambda x: len(x.split()))\n",
        "    \n",
        "    booleans = [(False, False, False), (False, False, True), (True, False, False), (True, False, True), (False, True, False),\n",
        "                (False, True, True)]\n",
        "    for i in range(len(booleans)):\n",
        "        df[feature_engg_cols[i]] = df[['clean_title', 'clean_question_body']].apply(\n",
        "        func = text_features, axis = 1, args = booleans[i])\n",
        "    \n",
        "    for i in range(len(booleans)):\n",
        "        df[feature_engg_cols[i+6]] = df[['clean_title', 'clean_answer']].apply(\n",
        "        func = text_features, axis = 1, args = booleans[i])\n",
        "\n",
        "    for i in range(len(booleans)):\n",
        "        df[feature_engg_cols[i+12]] = df[['clean_question_body', 'clean_answer']].apply(\n",
        "        func = text_features, axis = 1, args = booleans[i])\n",
        "\n",
        "\n",
        "    df['word_title_question_body_ratio'] = df[['clean_title', 'clean_question_body']].apply(\n",
        "        func = get_ratio, axis = 1, args = (False, ))\n",
        "    df['token_title_question_body_ratio'] = df[['clean_title', 'clean_question_body']].apply(\n",
        "        func = get_ratio, axis = 1, args = (True, ))\n",
        "    df['word_question_body_answer_ratio'] = df[['clean_question_body', 'clean_answer']].apply(\n",
        "        func = get_ratio, axis = 1, args = (False, ))\n",
        "    df['token_question_body_answer_ratio'] = df[['clean_question_body', 'clean_answer']].apply(\n",
        "        func = get_ratio, axis = 1, args = (True, ))\n",
        "    df['word_title_answer_ratio'] = df[['clean_title', 'clean_answer']].apply(func = get_ratio, \n",
        "                                                    axis = 1, args = (False, ))\n",
        "    df['token_title_answer_ratio'] = df[['clean_title', 'clean_answer']].apply(func = get_ratio, \n",
        "                                                    axis = 1, args = (True, ))\n",
        "\n",
        "    df['ratio_title_question_body'] = \\\n",
        "    df[['clean_title', 'clean_question_body']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (True, False, False, False))\n",
        "    df['partial_ratio_title_question_body'] = \\\n",
        "    df[['clean_title', 'clean_question_body']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, True, False, False))\n",
        "    df['token_sort_ratio_title_question_body'] = \\\n",
        "    df[['clean_title', 'clean_question_body']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, True, False))\n",
        "    df['token_set_ratio_title_question_body'] = \\\n",
        "    df[['clean_title', 'clean_question_body']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, False, True))\n",
        "    df['ratio_question_body_answer'] = \\\n",
        "    df[['clean_question_body', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (True, False, False, False ))\n",
        "    df['partial_ratio_question_body_answer'] = \\\n",
        "    df[['clean_question_body', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, True, False, False))\n",
        "    df['token_sort_ratio_question_body_answer'] = \\\n",
        "    df[['clean_question_body', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, True, False))\n",
        "    df['token_set_ratio_question_body_answer'] = \\\n",
        "    df[['clean_question_body', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, False, True))\n",
        "    df['ratio_title_answer'] = \\\n",
        "    df[['clean_title', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (True, False, False, False))\n",
        "    df['partial_ratio_title_answer'] = \\\n",
        "    df[['clean_title', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, True, False, False))\n",
        "    df['token_sort_ratio_title_answer'] = \\\n",
        "    df[['clean_title', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, True, False))\n",
        "    df['token_set_ratio_title_answer'] = \\\n",
        "    df[['clean_title', 'clean_answer']].apply(func = get_fuzzy_ratio, \n",
        "                                                     axis = 1, args = (False, False, False, True))\n",
        "    \n",
        "    \n",
        "    \n",
        "    df['clean_t_q'] = df['clean_title'] + ' ' + df['clean_question_body']\n",
        "    \n",
        "    data = {}\n",
        "    \n",
        "    data['xlnet_t_a'] = {}\n",
        "    data['xlnet_q_a'] = {}\n",
        "    data['xlnet_t_q'] = {}\n",
        "    data['xlnet_q'] = {}\n",
        "    data['xlnet_a'] = {}\n",
        "\n",
        "    tags = ['input_ids', 'token_type_ids', 'attention_masks']\n",
        "    data['xlnet_t_a'][tags[0]], data['xlnet_t_a'][tags[1]],data['xlnet_t_a'][tags[2]] = \\\n",
        "    [], [], []\n",
        "    data['xlnet_q_a'][tags[0]], data['xlnet_q_a'][tags[1]], data['xlnet_q_a'][tags[2]] = \\\n",
        "    [], [], []\n",
        "    data['xlnet_t_q'][tags[0]], data['xlnet_t_q'][tags[1]], data['xlnet_t_q'][tags[2]] = \\\n",
        "    [], [], []\n",
        "    data['xlnet_q'][tags[0]], data['xlnet_q'][tags[1]], data['xlnet_q'][tags[2]] = \\\n",
        "    [], [], []\n",
        "    data['xlnet_a'][tags[0]], data['xlnet_a'][tags[1]], data['xlnet_a'][tags[2]] = \\\n",
        "    [], [], []\n",
        "    \n",
        "    \n",
        "    \n",
        "    for i in range(df.shape[0]):\n",
        "        tokenize_input(xlnet_tokenizer, df.iloc[i].question_title, \n",
        "                       df.iloc[i].answer, tags, 'xlnet_t_a', 512)\n",
        "        tokenize_input(xlnet_tokenizer, df.iloc[i].question_body, \n",
        "                       df.iloc[i].answer, tags, 'xlnet_q_a', 512)\n",
        "        tokenize_input(xlnet_tokenizer, df.iloc[i].question_title, \n",
        "                       df.iloc[i].question_body, tags, 'xlnet_t_q', 512)\n",
        "        tokenize_input(xlnet_tokenizer, df.iloc[i].question_body, None, tags, 'xlnet_q', 512)\n",
        "        tokenize_input(xlnet_tokenizer, df.iloc[i].answer, None, tags, 'xlnet_a', 512)\n",
        "    for key, _ in data.items():\n",
        "        for k, _ in data[key].items():\n",
        "            data[key][k] = np.array(data[key][k])\n",
        "\n",
        "    MAX_LENGTH = 512\n",
        "    data['question_title'] = get_token_ids(df['clean_t_q'], MAX_LENGTH)\n",
        "    data['answer'] = get_token_ids(df['clean_answer'], MAX_LENGTH)\n",
        "    \n",
        "    fe = df[feature_engg_cols]\n",
        "    \n",
        "    df['combine_title_question'] = df['question_title'] + ' ' + df['question_body']\n",
        "\n",
        "    data['question_title_use'] = []\n",
        "    data['answer_use'] = []\n",
        "\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    for i in range(0, df.shape[0], BATCH_SIZE):\n",
        "        data['question_title_use'] += [encoder.signatures['question_encoder'](\n",
        "            \n",
        "          tf.constant(df['combine_title_question'].iloc[i:i+BATCH_SIZE].tolist())\n",
        "        \n",
        "        )['outputs'].numpy().astype(np.float16)]\n",
        "        data['answer_use'] += [encoder.signatures['response_encoder'](\n",
        "            \n",
        "        input = tf.constant(df['answer'].iloc[i:i+BATCH_SIZE].tolist()), \n",
        "        context = tf.constant(df['answer'].iloc[i:i+BATCH_SIZE].tolist())\n",
        "        \n",
        "        )['outputs'].numpy().astype(np.float16)]\n",
        "\n",
        "    data['question_title_use'] = np.vstack(data['question_title_use'])\n",
        "    data['answer_use'] = np.vstack(data['answer_use'])\n",
        "\n",
        "    \n",
        "    return data, fe"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMvp6vAQVzi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def function1(df):\n",
        "    '''Takes input and outputs prediction'''\n",
        "\n",
        "    tags = ['input_ids', 'token_type_ids', 'attention_masks']\n",
        "    target_cols = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', \n",
        "               'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', \n",
        "               'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', \n",
        "               'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', \n",
        "               'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', \n",
        "               'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', \n",
        "               'answer_well_written']\n",
        "    data, fe = get_inputs(df)\n",
        "    c_lstm_inputs = (data['question_title'], data['answer'],\n",
        "                   data['question_title_use'], data['answer_use'], fe)\n",
        "    \n",
        "    xlnet_inputs = (\n",
        "                    data['xlnet_a'][tags[0]], data['xlnet_t_q'][tags[0]], \n",
        "        data['xlnet_a'][tags[1]], data['xlnet_t_q'][tags[1]],\n",
        "                   data['xlnet_a'][tags[2]], data['xlnet_t_q'][tags[2]]  \n",
        "                \n",
        "                 )\n",
        "    xlnet_predictions = xlnet_model.predict(xlnet_inputs)\n",
        "    c_lstm_predictions = c_lstm_model.predict(c_lstm_inputs)\n",
        "    predictions = 0.5*(xlnet_predictions + c_lstm_predictions)\n",
        "    predictions[:, 19] = np.multiply(predictions[:, 19], df['question_type_spelling_modified'].values)\n",
        "    predictions = opt.transform(predictions)\n",
        "    output = {}\n",
        "    for i in range(30):\n",
        "        output[target_cols[i]] = predictions[:, i]\n",
        "    return pd.DataFrame(output)\n",
        "\n",
        "\n",
        "def function2(df):\n",
        "    '''Takes input and outputs Spearman Correlation Coefficient as a metric'''\n",
        "    \n",
        "    tags = ['input_ids', 'token_type_ids', 'attention_masks']\n",
        "    target_cols = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', \n",
        "               'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', \n",
        "               'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', \n",
        "               'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', \n",
        "               'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', \n",
        "               'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', \n",
        "               'answer_well_written']\n",
        "    data, fe = get_inputs(df)\n",
        "    c_lstm_inputs = (data['question_title'], data['answer'],\n",
        "                   data['question_title_use'], data['answer_use'], fe)\n",
        "    \n",
        "    xlnet_inputs = (\n",
        "                    data['xlnet_a'][tags[0]], data['xlnet_t_q'][tags[0]], \n",
        "        data['xlnet_a'][tags[1]], data['xlnet_t_q'][tags[1]],\n",
        "                   data['xlnet_a'][tags[2]], data['xlnet_t_q'][tags[2]]  \n",
        "                \n",
        "                 )\n",
        "    true_labels = df[target_cols].values.astype(np.float32) \n",
        "    xlnet_predictions = xlnet_model.predict(xlnet_inputs)\n",
        "    c_lstm_predictions = c_lstm_model.predict(c_lstm_inputs)\n",
        "    predictions = 0.5*(xlnet_predictions + c_lstm_predictions)\n",
        "    predictions[:, 19] = np.multiply(1, df['question_type_spelling_modified'].values)\n",
        "    scores = SpearmanCorrCoeff(predictions, true_labels)\n",
        "    print(f\"Spearman Corr Coeff (Before Optimizer): {np.round(np.mean(scores), 3)}\")\n",
        "    predictions = opt.transform(predictions)\n",
        "    scores = SpearmanCorrCoeff(predictions, true_labels)\n",
        "    print(f\"Spearman Corr Coeff (After Optimizer): {np.round(np.mean(scores), 3)}\\n\")\n",
        "    for index, target in enumerate(target_cols):\n",
        "        print(f\"{target} -----------------------------------------> {scores[index]}\")\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9qhk0SuNU2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.getcwd() != '/content/drive/My Drive/google_qa':\n",
        "    os.chdir('./drive/My Drive/google_qa')\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet/tokenizer')\n",
        "xlnet_model = transformer_model('xlnet')\n",
        "xlnet_model.load_weights('./final_models/model-save-weights/xlnet')\n",
        "c_lstm_model = tf.keras.models.load_model('final_models/c-lstm-save/c-lstm')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "file = np.load('embeddings.npz', allow_pickle = True)\n",
        "vocab = file['a'].tolist()\n",
        "embedding_matrix = file['b']\n",
        "encoder = hub.load('final_models/universal-sentence-encoder-qa_3')\n",
        "opt = pickle.load(open('optimize', 'rb'))\n",
        "train_df = pd.read_csv('train.csv')\n",
        "gkf = GroupKFold(n_splits = 5).split(X=train_df.url, groups = train_df.url)\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "    if fold == 2:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqro3196TUHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "c24899da-0388-42d3-de36-ac89af042bb7"
      },
      "source": [
        "inputs = {}\n",
        "inputs['question_title'] = ['Hello guys i am following an android tutorial online i wrote my code, but the app keeps crashing please help me']\n",
        "inputs['question_body'] = ['''Hello guys i am following an android tutorial online i wrote my code, \n",
        "but the app keeps crashing please help me Below is the code here\\'s the screenshot please bear \n",
        "with me i am a newbe, and oh am using android studio 4.0.1 i even tried writing the code from scratch but still \n",
        "got same error message over and over.''']\n",
        "inputs['answer'] = ['''You should fix your erasing type parameters in some new type. Type alias is not a new type, is just an additional name for the current type.\n",
        "\n",
        "You can do something like this:\n",
        "\n",
        "trait Resource\n",
        "\n",
        "class templatedResource[T](t: T) extends Resource\n",
        "\n",
        "case class BParameter()\n",
        "case class CParameter()\n",
        "\n",
        "object B {\n",
        "  case class Resource(val b: BParameter) extends templatedResource[BParameter](b)\n",
        "}\n",
        "\n",
        "object C {\n",
        "  type Resource = templatedResource[CParameter]\n",
        "}\n",
        "\n",
        "def process(r: Resource) = {\n",
        "  r match {\n",
        "    case a: B.Resource => true\n",
        "  }\n",
        "}\n",
        "\n",
        "process(B.Resource(BParameter()))\n",
        "If you need to preserve the creation syntax val bResource: B.Resource = templatedResource(bParam) in order to eliminate boilerplate for end-user - you should define the function with such creation. To eliminate implementation boilerplate of the function - you can use macro or something like shapeless, I guess.''']\n",
        "inputs['host'] = ['photo.stackexchange.com']\n",
        "df = pd.DataFrame(inputs)\n",
        "ans = function1(df)\n",
        "ans.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying Preprocessing.....\n",
            "Preprocessing Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_asker_intent_understanding</th>\n",
              "      <th>question_body_critical</th>\n",
              "      <th>question_conversational</th>\n",
              "      <th>question_expect_short_answer</th>\n",
              "      <th>question_fact_seeking</th>\n",
              "      <th>question_has_commonly_accepted_answer</th>\n",
              "      <th>question_interestingness_others</th>\n",
              "      <th>question_interestingness_self</th>\n",
              "      <th>question_multi_intent</th>\n",
              "      <th>question_not_really_a_question</th>\n",
              "      <th>question_opinion_seeking</th>\n",
              "      <th>question_type_choice</th>\n",
              "      <th>question_type_compare</th>\n",
              "      <th>question_type_consequence</th>\n",
              "      <th>question_type_definition</th>\n",
              "      <th>question_type_entity</th>\n",
              "      <th>question_type_instructions</th>\n",
              "      <th>question_type_procedure</th>\n",
              "      <th>question_type_reason_explanation</th>\n",
              "      <th>question_type_spelling</th>\n",
              "      <th>question_well_written</th>\n",
              "      <th>answer_helpful</th>\n",
              "      <th>answer_level_of_information</th>\n",
              "      <th>answer_plausible</th>\n",
              "      <th>answer_relevance</th>\n",
              "      <th>answer_satisfaction</th>\n",
              "      <th>answer_type_instructions</th>\n",
              "      <th>answer_type_procedure</th>\n",
              "      <th>answer_type_reason_explanation</th>\n",
              "      <th>answer_well_written</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.881528</td>\n",
              "      <td>0.552358</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>0.755049</td>\n",
              "      <td>0.657136</td>\n",
              "      <td>0.847976</td>\n",
              "      <td>0.578685</td>\n",
              "      <td>0.387877</td>\n",
              "      <td>0.015265</td>\n",
              "      <td>0.009695</td>\n",
              "      <td>0.671923</td>\n",
              "      <td>0.017431</td>\n",
              "      <td>0.003652</td>\n",
              "      <td>0.001004</td>\n",
              "      <td>0.002025</td>\n",
              "      <td>0.011693</td>\n",
              "      <td>0.768963</td>\n",
              "      <td>0.139966</td>\n",
              "      <td>0.412659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.638099</td>\n",
              "      <td>0.959799</td>\n",
              "      <td>0.587193</td>\n",
              "      <td>0.972681</td>\n",
              "      <td>0.982002</td>\n",
              "      <td>0.91427</td>\n",
              "      <td>0.865326</td>\n",
              "      <td>0.174745</td>\n",
              "      <td>0.274657</td>\n",
              "      <td>0.906001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_asker_intent_understanding  ...  answer_well_written\n",
              "0                             0.881528  ...             0.906001\n",
              "\n",
              "[1 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlb-QNKNNen0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "5e900909-9db9-4d30-c6b8-67bcd6416e92"
      },
      "source": [
        "t0 = time.time()\n",
        "function2(train_df.iloc[valid_idx])\n",
        "print(f'Time taken : {np.round((time.time() - t0)/60, 3)} mins')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying Preprocessing.....\n",
            "Preprocessing Finished\n",
            "Spearman Corr Coeff (Before Optimizer): 0.424\n",
            "Spearman Corr Coeff (After Optimizer): 0.442\n",
            "\n",
            "question_asker_intent_understanding -----------------------------------------> 0.36167801327441984\n",
            "question_body_critical -----------------------------------------> 0.6319027949072041\n",
            "question_conversational -----------------------------------------> 0.532750502791162\n",
            "question_expect_short_answer -----------------------------------------> 0.3316063017710274\n",
            "question_fact_seeking -----------------------------------------> 0.3874655769317777\n",
            "question_has_commonly_accepted_answer -----------------------------------------> 0.5358248959260236\n",
            "question_interestingness_others -----------------------------------------> 0.3686050954822212\n",
            "question_interestingness_self -----------------------------------------> 0.48392884201162084\n",
            "question_multi_intent -----------------------------------------> 0.6243872349447608\n",
            "question_not_really_a_question -----------------------------------------> 0.0883102020145098\n",
            "question_opinion_seeking -----------------------------------------> 0.48256239001388335\n",
            "question_type_choice -----------------------------------------> 0.7466926160621521\n",
            "question_type_compare -----------------------------------------> 0.4359922052948492\n",
            "question_type_consequence -----------------------------------------> 0.15204885219466655\n",
            "question_type_definition -----------------------------------------> 0.6487949520645051\n",
            "question_type_entity -----------------------------------------> 0.5605126792016213\n",
            "question_type_instructions -----------------------------------------> 0.769322453518257\n",
            "question_type_procedure -----------------------------------------> 0.34494993839759935\n",
            "question_type_reason_explanation -----------------------------------------> 0.669996553712261\n",
            "question_type_spelling -----------------------------------------> 0.4176527619820004\n",
            "question_well_written -----------------------------------------> 0.4895501929971406\n",
            "answer_helpful -----------------------------------------> 0.24793528800825876\n",
            "answer_level_of_information -----------------------------------------> 0.4328439765289985\n",
            "answer_plausible -----------------------------------------> 0.12424173266379628\n",
            "answer_relevance -----------------------------------------> 0.13674513081559878\n",
            "answer_satisfaction -----------------------------------------> 0.3455503255574019\n",
            "answer_type_instructions -----------------------------------------> 0.7781265579199377\n",
            "answer_type_procedure -----------------------------------------> 0.3167436705664278\n",
            "answer_type_reason_explanation -----------------------------------------> 0.6924747543536179\n",
            "answer_well_written -----------------------------------------> 0.1325815852275583\n",
            "Time taken : 5.273 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZRWK6RYfdzq",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "* https://www.kaggle.com/sakami/google-quest-single-lstm?scriptVersionId=28487242 - The CNN-LSTM takes some inspiration from this kernel\n",
        "* https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings - Preprocessing of text data is inspired from this kernel\n",
        "* https://huggingface.co/ - Documentation for Huggingface Library\n",
        "* https://www.kaggle.com/c/google-quest-challenge/discussion/130041 - Used the hack for question_type_spelling\n"
      ]
    }
  ]
}